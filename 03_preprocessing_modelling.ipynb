{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbdd9af",
   "metadata": {},
   "source": [
    "# Project 3 - Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12435df",
   "metadata": {},
   "source": [
    "## Part Three : Preprocessing & Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35351603",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "\n",
    "* Import Librairies\n",
    "* Load merged data\n",
    "* Preprocessing\n",
    "    - Mapping the y Variable\n",
    "    - NLP Preprocessing\n",
    "        - Lemmatization\n",
    "        - Stemming\n",
    "        - Stopwords\n",
    "        - Outcomes from NLP Preprocessing\n",
    "    - Creating X feature and y\n",
    "    - Train-test Split\n",
    "    - Determing the Baseline Score\n",
    "* Modeling\n",
    "    - Logistic Regression Model\n",
    "        - LR: Confusion Matrix\n",
    "        - LR: Interpretation\n",
    "    - Bernoulli NB Model\n",
    "        - NB: Confusion Matrix\n",
    "        - NB: Interpretation\n",
    "    - Bagged Decision Tree\n",
    "        - BDT: Confusion Matrix\n",
    "        - BDT: Interpretation\n",
    "    - Random Forest Model\n",
    "        - RF: Confusion Matrix\n",
    "        - RF: Interpretation\n",
    "* Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216c912",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdb07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#graphing \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#using nlp \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "#interpretation on the models \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe6814",
   "metadata": {},
   "source": [
    "### Load merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726bf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = pd.read_csv('./datasets/subreddit_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3191f",
   "metadata": {},
   "source": [
    "Check if data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efeae5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have any advice for the soon to be newl...</td>\n",
       "      <td>dreamingonastar1</td>\n",
       "      <td>1658560509</td>\n",
       "      <td>I learn more and more every day about the pers...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tp type personality pattern (ex:soccer)</td>\n",
       "      <td>Real_Marsupial8984</td>\n",
       "      <td>1658560467</td>\n",
       "      <td>Estp: Experience (Se) first and then create yo...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>May Pang</td>\n",
       "      <td>depressedgod13</td>\n",
       "      <td>1658559322</td>\n",
       "      <td>John Lennon’s temporary beau.\\n\\n[View Poll](h...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which type is the most “neutral” on their beli...</td>\n",
       "      <td>Hydra-Sagaria</td>\n",
       "      <td>1658558060</td>\n",
       "      <td>\\n\\n[View Poll](https://www.reddit.com/poll/w5...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does this seem Si dom?</td>\n",
       "      <td>akuasrA</td>\n",
       "      <td>1658557876</td>\n",
       "      <td>Do these traits seem to fit with the definitio...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              author  \\\n",
       "0  Do you have any advice for the soon to be newl...    dreamingonastar1   \n",
       "1            Tp type personality pattern (ex:soccer)  Real_Marsupial8984   \n",
       "2                                           May Pang      depressedgod13   \n",
       "3  Which type is the most “neutral” on their beli...       Hydra-Sagaria   \n",
       "4                             Does this seem Si dom?             akuasrA   \n",
       "\n",
       "   created_utc                                           selftext subreddit  \n",
       "0   1658560509  I learn more and more every day about the pers...      mbti  \n",
       "1   1658560467  Estp: Experience (Se) first and then create yo...      mbti  \n",
       "2   1658559322  John Lennon’s temporary beau.\\n\\n[View Poll](h...      mbti  \n",
       "3   1658558060  \\n\\n[View Poll](https://www.reddit.com/poll/w5...      mbti  \n",
       "4   1658557876  Do these traits seem to fit with the definitio...      mbti  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d317ab",
   "metadata": {},
   "source": [
    "#### Remove unnecessary column as we only need title and subreddit for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f14283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit=subreddit.drop(['created_utc','author','selftext'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7080b7",
   "metadata": {},
   "source": [
    "Let's check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9121aa06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have any advice for the soon to be newl...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tp type personality pattern (ex:soccer)</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>May Pang</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which type is the most “neutral” on their beli...</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does this seem Si dom?</td>\n",
       "      <td>mbti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title subreddit\n",
       "0  Do you have any advice for the soon to be newl...      mbti\n",
       "1            Tp type personality pattern (ex:soccer)      mbti\n",
       "2                                           May Pang      mbti\n",
       "3  Which type is the most “neutral” on their beli...      mbti\n",
       "4                             Does this seem Si dom?      mbti"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f532c0",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Before modeling, I need to do some preparation. First, I need to do addtional NLP prepocessing on my merged dataset to detemine if I want to use it in my modeling. Then, I create our X feature and y. Then, I will detemine the baseline. Lastly, do train-test split on the X fetaure and y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706c57e",
   "metadata": {},
   "source": [
    "#### Mapping the y Variable\n",
    "I change the target variable to a binary classifier so that it can be able to model it as a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2a0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the replace method to fix target values in the subreddits_titles data set\n",
    "subreddit['subreddit'].replace({'mbti': 1, 'astrology': 0}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff52abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1779\n",
       "1    1325\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca07c06",
   "metadata": {},
   "source": [
    "The target variable has been successfully changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064a7f7",
   "metadata": {},
   "source": [
    "#### NLP Preprocessing\n",
    "When dealing with text data, there are common pre-processing steps. I will tokenize, lemmatize, and stem. I will also use stopwords to clean up our text data. Finally, I can use two of these functions and use the results to model. I will either pick lemmatization and stopwords or stemming and stopwords.\n",
    "\n",
    "Lets create the text data and call it token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6a32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting our text data and using the copy method to unchange our dataframe feature \n",
    "token = subreddit['title'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14081111",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "When I lemmatize the text data, I take words in the data and attempt to return their base form of that word. Before I lemmatize, I need to tokenize the data. In other words, it will split up the data into distinct chunks based on a pattern of our choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d25bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'[A-z]+') #using the [A-z] pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793c9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run tokenizer, use join method to get our post to become lowercase\n",
    "#Created with Noah C. \n",
    "tokens = [\" \".join(tokenizer.tokenize(post.lower())) for post in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a6f129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do you have any advice for the soon to be newly weds intj lt isfp',\n",
       " 'tp type personality pattern ex soccer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see if it worked\n",
    "tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe126fd",
   "metadata": {},
   "source": [
    "It works, so let's move on to use lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "141b90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5a99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the split method to split our posts and lemmatize individual words in each post \n",
    "#Created with Noah C. \n",
    "tokens_lem = [[lemmatizer.lemmatize(word) for word in post.split(' ')] for post in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8609a01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do you have any advice for the soon to be newly weds intj lt isfp',\n",
       "  ['do',\n",
       "   'you',\n",
       "   'have',\n",
       "   'any',\n",
       "   'advice',\n",
       "   'for',\n",
       "   'the',\n",
       "   'soon',\n",
       "   'to',\n",
       "   'be',\n",
       "   'newly',\n",
       "   'wed',\n",
       "   'intj',\n",
       "   'lt',\n",
       "   'isfp']),\n",
       " ('tp type personality pattern ex soccer',\n",
       "  ['tp', 'type', 'personality', 'pattern', 'ex', 'soccer'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see the differences between tokens and lemmatize tokens \n",
    "list(zip(tokens, tokens_lem))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db77a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize tokens, use the join method to get our post to become lemmatize\n",
    "#Created with Noah C.\n",
    "lem_tokens = [\" \".join(post) for post in tokens_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef919f",
   "metadata": {},
   "source": [
    "Next, CountVectorizer is used to transform the lists of the cleaned posts into features that we can pass into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e2028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate CountVectorizer\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e904d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit lemmatize features \n",
    "cvec.fit_transform(lem_tokens);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2018a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature names\n",
    "vocab_lemma = cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c84a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with our data lemmatize\n",
    "vocab_lemma_df = pd.DataFrame(vocab_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68ab715c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0      _____\n",
       "1      aaron\n",
       "2         ab\n",
       "3  abducting\n",
       "4        abe"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if it worked \n",
    "vocab_lemma_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f29a06",
   "metadata": {},
   "source": [
    "I am able to lemmatize our X feature. Next, it is to stem X feature and it is very similar to lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d588f",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "Again, when I stem the text data, I take words in the data and attempt to return their base form of that word. Also, before stemming, we need to tokenize our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d9a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate object of class PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361031b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem tokens, use the split method to split our posts and stem individual words in each post \n",
    "#Created with Noah C.\n",
    "tokens_stem = [[p_stemmer.stem(word) for word in post.split(' ')] for post in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2db64aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do you have any advice for the soon to be newly weds intj lt isfp',\n",
       "  ['do',\n",
       "   'you',\n",
       "   'have',\n",
       "   'ani',\n",
       "   'advic',\n",
       "   'for',\n",
       "   'the',\n",
       "   'soon',\n",
       "   'to',\n",
       "   'be',\n",
       "   'newli',\n",
       "   'wed',\n",
       "   'intj',\n",
       "   'lt',\n",
       "   'isfp']),\n",
       " ('tp type personality pattern ex soccer',\n",
       "  ['tp', 'type', 'person', 'pattern', 'ex', 'soccer'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the differences between tokens and stemmed tokens \n",
    "list(zip(tokens, tokens_stem))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd2fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmed tokens, use the join method to get our post to become stemmed\n",
    "#Created with Noah C.\n",
    "stemmed_tokens = [\" \".join(post) for post in tokens_stem]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464fd11",
   "metadata": {},
   "source": [
    "Again, CountVectorizer is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d368ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate CountVectorizer\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35cf89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit stem features \n",
    "cvec.fit_transform(stemmed_tokens);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5038b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature names\n",
    "vocab_stemmed = cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51387c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with our data lemmatize\n",
    "vocab_stemmed_df = pd.DataFrame(vocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfe46711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abduct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0   _____\n",
       "1   aaron\n",
       "2      ab\n",
       "3  abduct\n",
       "4     abe"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked \n",
    "vocab_stemmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e74f1d",
   "metadata": {},
   "source": [
    "I am able to stem our X feature.\n",
    "\n",
    "Thus, we observed that stemming is better than lemmatization because it is able to get the clear root word in our dataset. For instance, in lemmatization we get the word \"abducting\" and in stemming we get the word \"abduct\". \n",
    "\n",
    "Finally, we can determine our stop words that will not be included into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553506d",
   "metadata": {},
   "source": [
    "**Stopwords**\n",
    "\n",
    "Stopwords are very common words that are often removed because they amount to unnecessary information and removing them can dramatically speed things up in our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97736547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#english stopwords are the common stop words in english\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0c6ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create another copy of our dataframe so it will not effect our modeling\n",
    "copy_st = subreddit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "493e45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing our title feature to a lower case and split the posts into indvidual words\n",
    "copy_st['title'] = copy_st['title'].str.lower().str.split()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99e65d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may', 'pang']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see if it worked\n",
    "copy_st['title'][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc3049a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if each word is either a stopword or not, if then drop word, if not keep word\n",
    "using_stop_words = copy_st['title'].apply(lambda post: [word for word in post if word not in stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "750a02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe with stopwords gone\n",
    "using_stop_words_df = pd.DataFrame(using_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7491d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[advice, soon, newly, weds?, intj, &amp;lt;3, isfp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tp, type, personality, pattern, (ex:soccer)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[may, pang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[type, “neutral”, beliefs, life?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[seem, si, dom?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title\n",
       "0  [advice, soon, newly, weds?, intj, &lt;3, isfp]\n",
       "1    [tp, type, personality, pattern, (ex:soccer)]\n",
       "2                                      [may, pang]\n",
       "3                [type, “neutral”, beliefs, life?]\n",
       "4                                 [seem, si, dom?]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked\n",
    "using_stop_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c96e6",
   "metadata": {},
   "source": [
    "I am able to determine our stopwords our X feature.\n",
    "\n",
    "After exploring each of the NLP functions, we will use lemmatisation and stopwords for our model. As mentioned, stemming would be better in general but for my case, stemming have removed most of the 'e' from my words and made most of my words unreadable, like advice to advic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee91b40",
   "metadata": {},
   "source": [
    "**Outcomes from NLP Preprocessing**\n",
    "\n",
    "Lets create our lemmatise X feature for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e45d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting our text data in our dataframe\n",
    "token_model = subreddit['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27ffc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate tokenizer\n",
    "tokenizer_model = RegexpTokenizer(r'[A-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05bf0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run tokenizer, use join method to get our post to become lowercase\n",
    "#Created with Noah C. \n",
    "tokens_model = [\" \".join(tokenizer_model.tokenize(post.lower())) for post in token_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad5055df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c431e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the split method to split our posts and lemmatize individual words in each post \n",
    "#Created with Noah C. \n",
    "tokens_lem_model= [[lemmatizer.lemmatize(word) for word in post.split(' ')] for post in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43077e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize tokens, use the join method to get our post to become lemmatize\n",
    "#Created with Noah C.\n",
    "lem_tokens_model = [\" \".join(post) for post in tokens_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f86cac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace our text data with new lemmatized data\n",
    "subreddit['title'] = lem_tokens_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c734f125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do you have any advice for the soon to be newl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tp type personality pattern ex soccer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>may pang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which type is the most neutral on their belief...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doe this seem si dom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i want an infj best friend female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abrasiveness and the perceiving ax</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>which type have you noticed isfj s tend to end...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>which type is more likely to be the victim of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what do you think</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>again te v fe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>type most likely to continue to hang out with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>your mbti and a genre you re fond of</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>isfp boy and intp girl relationship</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>im stuck in what is my mbti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>intj v infj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stacy from fast time at ridgemont high isfj or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>would you ever lie about your personality type</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>which type would brag the hardest to everyone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>type most likely to suggest that an isfj is hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>infjs in fiction v infjs irl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>could an isfj be very spacey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>do other ppl do this to u too</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>type most likely to find the troll post in r m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>what do you think</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>what do you think she is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>what do you think her type is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>what are the difference between enfp girl and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>do you believe in soulmates</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  subreddit\n",
       "0   do you have any advice for the soon to be newl...          1\n",
       "1               tp type personality pattern ex soccer          1\n",
       "2                                            may pang          1\n",
       "3   which type is the most neutral on their belief...          1\n",
       "4                                doe this seem si dom          1\n",
       "5                   i want an infj best friend female          1\n",
       "6                  abrasiveness and the perceiving ax          1\n",
       "7   which type have you noticed isfj s tend to end...          1\n",
       "8   which type is more likely to be the victim of ...          1\n",
       "9                                   what do you think          1\n",
       "10                                      again te v fe          1\n",
       "11  type most likely to continue to hang out with ...          1\n",
       "12                                           question          1\n",
       "13               your mbti and a genre you re fond of          1\n",
       "14                isfp boy and intp girl relationship          1\n",
       "15                        im stuck in what is my mbti          1\n",
       "16                                        intj v infj          1\n",
       "17  stacy from fast time at ridgemont high isfj or...          1\n",
       "18     would you ever lie about your personality type          1\n",
       "19  which type would brag the hardest to everyone ...          1\n",
       "20  type most likely to suggest that an isfj is hi...          1\n",
       "21                       infjs in fiction v infjs irl          1\n",
       "22                       could an isfj be very spacey          1\n",
       "23                      do other ppl do this to u too          1\n",
       "24  type most likely to find the troll post in r m...          1\n",
       "25                                  what do you think          1\n",
       "26                           what do you think she is          1\n",
       "27                      what do you think her type is          1\n",
       "28  what are the difference between enfp girl and ...          1\n",
       "29                        do you believe in soulmates          1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked\n",
    "subreddit.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71ea45",
   "metadata": {},
   "source": [
    "lemmatise X feature is created and going to be using in the model.\n",
    "\n",
    "Next, lets state the X feature and target variable.\n",
    "\n",
    "### Creating X feature and y\n",
    "Create our feature matrix (X) and target vector (y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70990e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = subreddit['title']\n",
    "y = subreddit['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3966011",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cee64eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f81fd056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3057                          chart ruler in the th house\n",
       "707                    which type hate ambiguity the most\n",
       "2434                            capricorn woman virgo man\n",
       "968     never be able to say i love you v never hear i...\n",
       "886     i m an entj and i don t know how to have fun a...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked \n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c354df",
   "metadata": {},
   "source": [
    "Finally, we can determine our baseline score.\n",
    "\n",
    "### Determing the Baseline Score\n",
    "The baseline score is actually an accuracy score. It is the percentage of the majority class, regardless of whether our y is 1 or 0. It will serve as the benchmark for our classification models to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c208d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.572683\n",
       "1    0.427317\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440aa9b",
   "metadata": {},
   "source": [
    "The majority class end up being r/astrology = 0 because this subreddit has more posts included in y. Therefore, our baseline accuracy is about 0.57. And each time we see a post, it is predicted to be r/astrology.\n",
    "\n",
    "Let's move on to modelling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a0897",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "I am going to use four different classification models to determine how accurate it is towards our problem statement with it's accuracy score. We will model: Logistic Regression, Bernoulli Naive Bayes, Bagged Decision Tree, and Random Forest.\n",
    "\n",
    "For Logistic Regression and Bernoulli Naive Bayes, we will use TfidfVectorizer as a transformer. TfidfVectorizer is another classification model that penalizes words in our dataset that are common across all posts in the corpus.\n",
    "\n",
    "I determine parameters for Logistic Regression and Random Forest models. A parameter sets the conditions of its models.\n",
    "\n",
    "Additionally, I need to transform our Logistic Regression and Random Forest models to GridSearchCV model. GridSearchCV helps us tune our parameters.\n",
    "\n",
    "Then I will supplement each model with there repected confusion matrix. Based on the confusion matrix, I am able to determine what was correctly classified and misclassified. This is important because it determines the second half of our problem statement. In other words, we want to see if each model can correctly classify where each post came from in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ea842",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model\n",
    "Logistic regression is the most common binary classification algorithm. The goal of our logistic regression model is to describe the relationship between our binary outcome which is our target variable and an independent variable which is our X feature.\n",
    "\n",
    "We will begin by instantiating a pipeline with TfidfVectorizer and LogisticRegression. Then, we will determine our parameters for our pipeline. Then, we will transform our model to a GridSearchCV. Then, we will fit our training dataset. Finally, we will able to get our training and testing accuracy scores.\n",
    "\n",
    "For our supplemental piece, we will create a confusion matrix.\n",
    "\n",
    "Instantiatie Pipeline with TfidfVectorizer() & LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11bd7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()), \n",
    "    ('lr', LogisticRegression(solver = 'liblinear', #making sure we have our slover as liblinear\n",
    "                              random_state = 42))#creating the random state as 42\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21cc0e",
   "metadata": {},
   "source": [
    "Using the .get_params attribute, I am able to get the parameters for this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dccd5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#got our parameters by trial and error\n",
    "pipe_params = {\n",
    "    'vectorizer__max_features': [1000, 2500, 5000], #include the 𝑁 most popular vocabulary words in the corpus\n",
    "    'vectorizer__ngram_range': [(1,1), (1,2)],#capture 𝑛 -word phrases\n",
    "    \"vectorizer__norm\":         ['l1', 'l2'],#using Lasso and Ridge \n",
    "    \"vectorizer__stop_words\":   ['english'],#taking away the english stopwords\n",
    "    'lr__penalty':              ['l1', 'l2'],#using Lasso and Ridge \n",
    "    'lr__C':                    [2.0],#Inverse of regularization strength\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e4896",
   "metadata": {},
   "source": [
    "Transform to GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "887d0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, #use the data which is pipe\n",
    "                  param_grid = pipe_params, #using our custom parameters\n",
    "                  cv = 5)#cv 5 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767eba35",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "612b23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guiwaisiong/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/guiwaisiong/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed793f",
   "metadata": {},
   "source": [
    "Obtaining the best estimator and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80ed75ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920625579240037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 2.0,\n",
       " 'lr__penalty': 'l2',\n",
       " 'vectorizer__max_features': 1000,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__norm': 'l2',\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs.best_score_) \n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c7ada",
   "metadata": {},
   "source": [
    "The best model estimator is about 0.920."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37064d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model as gs_model.\n",
    "gs_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccf5fc",
   "metadata": {},
   "source": [
    "**Accuracy training score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60ea38ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.963924963924964"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c63799",
   "metadata": {},
   "source": [
    "The accuracy score on our training data set is about 0.963.\n",
    "\n",
    "**Accuracy testing score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f5dad69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9307317073170732"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c74fc3",
   "metadata": {},
   "source": [
    "The accuracy score on our testing data set is about 0.93.\n",
    "\n",
    "Next, lets create our confusion matrix.\n",
    "\n",
    "#### LR: Confusion Matrix\n",
    "The confusion matrix we will create our true values and predicted values.\n",
    "\n",
    "Getting our testing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27160a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "010a2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, # True values.\n",
    "                 preds_test)  # Predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd03b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted MBTI</th>\n",
       "      <th>Predicted Astrology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual MBTI</th>\n",
       "      <td>571</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Astrology</th>\n",
       "      <td>55</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted MBTI  Predicted Astrology\n",
       "Actual MBTI                  571                   16\n",
       "Actual Astrology              55                  383"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a data frame for cm\n",
    "pd.DataFrame(data = cm, \n",
    "             columns = ['Predicted MBTI', 'Predicted Astrology'],\n",
    "             index = ['Actual MBTI', 'Actual Astrology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29af62",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, we was able to determine that out of 1025 posts, 571 + 383 = 954 posts were correctly classified and 55 + 16 = 71 posts were misclassified.\n",
    "\n",
    "Finally, lets move on to our interpretation.\n",
    "\n",
    "#### LR: Interpretation\n",
    "Metrics:\n",
    "\n",
    "The training accuracy score is about 96.3 percent.\n",
    "\n",
    "The testing accuracy score is about 93 percent.\n",
    "\n",
    "The correctly classified posts is 954.\n",
    "\n",
    "The misclassified posts is 71.\n",
    "\n",
    "The training accuracy score is well with our training dataset. The RMSE of our training data has tiny amounts of residuals away from our target variable. Therefore, base on our testing accuracy score, this model will be able to fit well with unknown data if we still use these two subreddit sites.\n",
    "\n",
    "\n",
    "Next, lets model bernoulli nb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859ffa7",
   "metadata": {},
   "source": [
    "#### Bernoulli NB Model\n",
    "Bernoulli Naive Bayes is appropriate when our features are all 0/1 variables which is our target variable. We use this model because it's a very fast modeling algorithm and outperforms more complicated models.\n",
    "\n",
    "I will first use the transformer TfidfVectorizer. Then I will create a new dataframe with our transformer that is the bag of words. A bag of words represents discard grammar, order, and structure in the text but track occurrences. Then, I will use our transformer as our X_train and X_test. Then, I can finally instantiatie our BernoulliNB. Then, I will fit our training dataset. Finally, I will able to get our training and testing accuracy scores.\n",
    "\n",
    "For supplemental piece, I will also create a confusion matrix.\n",
    "\n",
    "Transformer is TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aec6c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3183df",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8014ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bag of words with dataframe \n",
    "data = pd.DataFrame(tvec.fit_transform(X_train).toarray(), #able to have strings into floats\n",
    "                  columns=tvec.get_feature_names())#make column names with data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "848bb023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_____</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abducting</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorbed</th>\n",
       "      <th>...</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourselves</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtuber</th>\n",
       "      <th>youtubers</th>\n",
       "      <th>yr</th>\n",
       "      <th>yt</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiacal</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _____  aaron   ab  abducting  ability  able  aboard  about  absolutely  \\\n",
       "0    0.0    0.0  0.0        0.0      0.0   0.0     0.0    0.0         0.0   \n",
       "1    0.0    0.0  0.0        0.0      0.0   0.0     0.0    0.0         0.0   \n",
       "2    0.0    0.0  0.0        0.0      0.0   0.0     0.0    0.0         0.0   \n",
       "3    0.0    0.0  0.0        0.0      0.0   0.0     0.0    0.0         0.0   \n",
       "4    0.0    0.0  0.0        0.0      0.0   0.0     0.0    0.0         0.0   \n",
       "\n",
       "   absorbed  ...  yourself  yourselves  youtube  youtuber  youtubers   yr  \\\n",
       "0       0.0  ...       0.0         0.0      0.0       0.0        0.0  0.0   \n",
       "1       0.0  ...       0.0         0.0      0.0       0.0        0.0  0.0   \n",
       "2       0.0  ...       0.0         0.0      0.0       0.0        0.0  0.0   \n",
       "3       0.0  ...       0.0         0.0      0.0       0.0        0.0  0.0   \n",
       "4       0.0  ...       0.0         0.0      0.0       0.0        0.0  0.0   \n",
       "\n",
       "    yt  zodiac  zodiacal  zombie  \n",
       "0  0.0     0.0       0.0     0.0  \n",
       "1  0.0     0.0       0.0     0.0  \n",
       "2  0.0     0.0       0.0     0.0  \n",
       "3  0.0     0.0       0.0     0.0  \n",
       "4  0.0     0.0       0.0     0.0  \n",
       "\n",
       "[5 rows x 2650 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if it worked\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4825c27",
   "metadata": {},
   "source": [
    "Transformer training and testing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3cf1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tvec.transform(X_train)\n",
    "\n",
    "X_test = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02520ba",
   "metadata": {},
   "source": [
    "Instantiatie the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bc74139",
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5314b2",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62406e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "br.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac0883",
   "metadata": {},
   "source": [
    "**Accuracy testing score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d482538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473170731707317"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4441a8",
   "metadata": {},
   "source": [
    "The accuracy score on our testing data set is about 0.947.\n",
    "\n",
    "**Accuracy training score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d8177e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788359788359788"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27653960",
   "metadata": {},
   "source": [
    "The accuracy score on our training data set is about 0.978.\n",
    "\n",
    "Next, lets create our confusion matrix.\n",
    "\n",
    "#### NB: Confusion Matrix\n",
    "Getting our testing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "986c8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_1 = br.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81b155ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_1 = confusion_matrix(y_test, # True values.\n",
    "                 preds_test_1)  # Predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb28ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted MBTI</th>\n",
       "      <th>Predicted astrology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual MBTI</th>\n",
       "      <td>566</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Astrology</th>\n",
       "      <td>33</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted MBTI  Predicted astrology\n",
       "Actual MBTI                  566                   21\n",
       "Actual Astrology              33                  405"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a data frame for cm_1\n",
    "pd.DataFrame(data = cm_1, \n",
    "             columns = ['Predicted MBTI', 'Predicted astrology'],\n",
    "             index = ['Actual MBTI', 'Actual Astrology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e197c",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, we was able to determine that out of 1025 posts, 971 posts were correctly classified and 54 posts were misclassified.\n",
    "\n",
    "Finally, lets move on to our interpretation.\n",
    "\n",
    "#### NB: Interpretation\n",
    "Metrics:\n",
    "\n",
    "The training accuracy score is about 97.4 percent.\n",
    "\n",
    "The testing accuracy score is about 94.8 percent.\n",
    "\n",
    "The correctly classified posts is 971.\n",
    "\n",
    "The misclassified posts is 54.\n",
    "\n",
    "The training accuracy score fits really well with our training dataset. The RMSE of our training data has small amount of residuals away from our target variable. Therefore, base on our testing accuracy score, this model will be able to fit well with unknown data if we still use these two subreddit sites.\n",
    "\n",
    "Next, lets model bagged decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dfc0b",
   "metadata": {},
   "source": [
    "#### Bagged Decision Tree\n",
    "Decision trees have limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns. Thus, we will be using a Bagged Decision Tree Model. BDT fixes this problem by exposing different trees to different sub-samples of the training set.\n",
    " \n",
    "First I instantiate the BaggingClassifier. Then, I will fit in training. Finally, I able to get our training and testing accuracy scores.\n",
    "\n",
    "For supplemental piece, I will also create a confusion matrix.\n",
    "\n",
    "Instantiate BaggingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4693730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(random_state = 42)#using random state 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72581a",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa5fd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da39e9",
   "metadata": {},
   "source": [
    "Accuracy training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e4b1386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913419913419913"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee67d3e",
   "metadata": {},
   "source": [
    "The accuracy score on our training data set is about 0.991.\n",
    "\n",
    "Accuracy testing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d41f77ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.895609756097561"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205abff",
   "metadata": {},
   "source": [
    "The accuracy score on our testing data set is about 0.895.\n",
    "\n",
    "Next, lets create our confusion matrix.\n",
    "\n",
    "#### BDT: Confusion Matrix\n",
    "Getting our testing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a9d63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_3 = bag.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f6cfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_3 = confusion_matrix(y_test, # True values.\n",
    "                 preds_test_3)  # Predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc1d8770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted MBTI</th>\n",
       "      <th>Predicted astrology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual MBTI</th>\n",
       "      <td>564</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Astrology</th>\n",
       "      <td>84</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted MBTI  Predicted astrology\n",
       "Actual MBTI                  564                   23\n",
       "Actual Astrology              84                  354"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a data frame for cm_3\n",
    "pd.DataFrame(data = cm_3, \n",
    "             columns = ['Predicted MBTI', 'Predicted astrology'],\n",
    "             index = ['Actual MBTI', 'Actual Astrology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c63c2",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, we was able to determine that out of 1025 posts, 918 posts were correctly classified and 107 posts were misclassified.\n",
    "\n",
    "Finally, lets move on to our interpretation.\n",
    "\n",
    "#### BDT: Interpretation\n",
    "\n",
    "Metrics:\n",
    "\n",
    "The training accuracy score is about 99.1 percent.\n",
    "\n",
    "The testing accuracy score is about 89.5 percent.\n",
    "\n",
    "The correctly classified posts is 918.\n",
    "\n",
    "The misclassified posts is 107.\n",
    "\n",
    "The training accuracy score fits extremely well with our training dataset. The RMSE of our training data has small amount of residuals away from our target variable. The testing accuracy score fits less with our training data. And the RMSE of our testing data has more residuals away from our target variable.\n",
    "\n",
    "\n",
    "Finally, lets model random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25367180",
   "metadata": {},
   "source": [
    "#### Random Forest Model\n",
    "Random Forest are very similar to Bagged Decision Trees. Yet, the difference is only a subset of features are selected at random out of the total. In other words, the best split feature from the subset is used to split each node in a tree.\n",
    "\n",
    "I begin by instantiating Random Forest. Then, I determine our parameters. Then, I will transform our model to a GridSearchCV. Then, I will fit the training dataset. Finally, I get our training and testing accuracy scores.\n",
    "\n",
    "For supplemental piece, I create a confusion matrix.\n",
    "\n",
    "Instantiatie RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a6961d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying find the best split point in our fetaures\n",
    "rf = RandomForestClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2923cd3",
   "metadata": {},
   "source": [
    "Using the .get_params attribute, we were able to get the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4dd01d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#got our parameters by trial and error\n",
    "rf_params = {\n",
    "    'n_estimators': [110, 150],\n",
    "    'max_depth': [None, 4, 6],\n",
    "    'criterion': ['gini']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a31521",
   "metadata": {},
   "source": [
    "Transform to GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aff1904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_2 = GridSearchCV(rf, param_grid=rf_params, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e798f",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f32c6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_2.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1472a",
   "metadata": {},
   "source": [
    "Obtaining the best estimator and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58f24008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9061943929564411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': None, 'n_estimators': 110}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs_2.best_score_) \n",
    "gs_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b104d",
   "metadata": {},
   "source": [
    "The best model estimator is about 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f3bc36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model_2 = gs_2.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de6d20",
   "metadata": {},
   "source": [
    "Accuracy training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c487a283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980759980759981"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model_2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a7c5e",
   "metadata": {},
   "source": [
    "The accuracy score on our training data set is about 0.998.\n",
    "\n",
    "Accuracy testing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0573041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9131707317073171"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d44da",
   "metadata": {},
   "source": [
    "The accuracy score on our testing data set is about 0.778.\n",
    "\n",
    "Next, lets create our confusion matrix.\n",
    "\n",
    "#### RFM: Confusion Matrix\n",
    "The confusion matrix we will create has our true values and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fab1b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_4 = gs_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14869743",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_4 = confusion_matrix(y_test, # True values.\n",
    "                 preds_test_4)  # Predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "efc3c610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted MBTI</th>\n",
       "      <th>Predicted Astrology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual MBTI</th>\n",
       "      <td>542</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Astrology</th>\n",
       "      <td>44</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted MBTI  Predicted Astrology\n",
       "Actual MBTI                  542                   45\n",
       "Actual Astrology              44                  394"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a data frame for cm_4\n",
    "pd.DataFrame(data = cm_4, \n",
    "             columns = ['Predicted MBTI', 'Predicted Astrology'],\n",
    "             index = ['Actual MBTI', 'Actual Astrology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f134d53",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, we was able to determine that out of 1025 posts, 936 posts were correctly classified and 89 posts were misclassified.\n",
    "\n",
    "Finally, lets move on to our interpretation.\n",
    "\n",
    "#### RF: Interpretation\n",
    "\n",
    "Metrics:\n",
    "\n",
    "The training accuracy score is about 99.8 percent.\n",
    "\n",
    "The testing accuracy score is about 91.3 percent.\n",
    "\n",
    "The correctly classified posts is 936.\n",
    "\n",
    "The misclassified posts is 89.\n",
    "\n",
    "Successes: The training accuracy score fits extremely well with our training dataset. The best out of all of my models. The RMSE of our training data has very small residuals away from our target variable. Therefore, base on our testing accuracy score, this model will be able to fit decently well with unknown data if we still use these two subreddit sites.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a13af2",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "\n",
    "I create fake data with 1000 rows samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e3721c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fake dataset\n",
    "X, y = datasets.make_classification(n_samples=1000,\n",
    "                                    n_features=4,\n",
    "                                    n_informative=3,\n",
    "                                    n_redundant=1,\n",
    "                                    random_state=0)\n",
    "\n",
    "#split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e3b8e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set up plotting area\n",
    "plt.figure(0).clf()\n",
    "\n",
    "#fit logistic regression model and plot ROC curve\n",
    "gs = LogisticRegression()\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"Logistic Regression, AUC=\"+str(auc))\n",
    "\n",
    "#fit Bernoulli NB and plot ROC curve\n",
    "br = BernoulliNB()\n",
    "br.fit(X_train, y_train)\n",
    "y_pred = br.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"Bernoulli NB, AUC=\"+str(auc))\n",
    "\n",
    "#fit Bagged Decision Tree and plot ROC curve\n",
    "bag = BaggingClassifier(random_state = 42)\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred = bag.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"Bagged Decision Tree, AUC=\"+str(auc))\n",
    "\n",
    "#fit Random forest and plot ROC curve\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"Random Forest, AUC=\"+str(auc))\n",
    "\n",
    "\n",
    "#add legend\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f79a1f",
   "metadata": {},
   "source": [
    "### Conclusion and Recommendations\n",
    "\n",
    "| | Model | Training Accuracy Score  | Testing Accuracy Score | Correctly Classified |Misclassified| AUC|\n",
    "|---:|:-------------|:-----------|:------|:------|:----------|:---------|\n",
    "| 1 | Logistic Regression | 0.963  | 0.930  | 945  | 71|0.7902|\n",
    "| 2 | Bernoulli Naive Bayes | 0.974  | 0.948 | 971 | 54|0.8437|\n",
    "| 3 | Bagged Decision Tree | 0.991|0.895|918|107|0.9799|\n",
    "| 4 | Random forest |0.998|0.913|936|89|0.9822|\n",
    "\n",
    "\n",
    "\n",
    "All the classification models: Logistic Regression, Bernoulli Naive Bayes, Bagged Decision Tree, and Random Forest surpassed the baseline accuracy. \n",
    "\n",
    "**Bernoulli Naive Bayes Classification Model** was the best model to test our training data because it was able to manage well with unknown data according to the testing accuracy score. Despite having lower AUC score than both bagged decision tree and random forest.\n",
    "\n",
    "However, the model was still overfit because of low bias and high variance.\n",
    "\n",
    "Despite the overfitting, this model was able to classified similar content from two different web sources: r/mbti and r/astrology. Also it was able to identify where each post came from which subreddit. It had one of the highest in correctly classifying posts which was 971. Therefore, Reddit will be able to implement this model for their studies on this data science topic.\n",
    "\n",
    "Yet, this model still had its limitations. Some of the posts had similar titles and incorrect spelling. It was not able to identify these mishaps because of NLP transformer. In other words, both of these mishaps could of swayed our model results.\n",
    "\n",
    "Also, our model can improve it's accuracy if we further tuned our hyperparameters. Additionally, we could of instantiate the PolynomialFeatures before our model to decrease the overfitting.\n",
    "\n",
    "So we still have some recommendations:\n",
    "\n",
    "- increase the stopword list with more nouns to have a better predictable model (i.e. ‘like’)\n",
    "- Each of the subreddits change over time. thus the result might change\n",
    "- a different model that we had not yet modeled (i.e SLM, Adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae9ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
